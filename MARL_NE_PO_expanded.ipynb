{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMdKKOjmjIa81gEECpbnjga"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0A3JSmbhedj",
        "outputId": "e5e38a31-1151-45f1-9bd2-dead744df15f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1432225906.py:11: DeprecationWarning: The environment `pettingzoo.mpe` has been moved to `mpe2` and will be removed in a future release.Please update your imports.\n",
            "  from pettingzoo.mpe import simple_spread_v3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "!pip install pettingzoo[classic,mpe] supersuit torch matplotlib seaborn pandas scipy > /dev/null 2>&1\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from pettingzoo.mpe import simple_spread_v3\n",
        "import random\n",
        "import os\n",
        "from google.colab import drive\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MatrixGame:\n",
        "    def __init__(self, payoffs, max_steps=50):\n",
        "        self.payoffs = torch.tensor(payoffs, dtype=torch.float32, device=device)\n",
        "        self.max_steps = max_steps\n",
        "        self.current_step = 0\n",
        "        self.n_actions = self.payoffs.shape[0]\n",
        "        self.last_actions = (0, 0)\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.last_actions = (0, 0)\n",
        "        return (torch.tensor([0.0], device=device), torch.tensor([0.0], device=device))\n",
        "\n",
        "    def step(self, a1, a2):\n",
        "        r1 = self.payoffs[a1, a2, 0]\n",
        "        r2 = self.payoffs[a1, a2, 1]\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.max_steps\n",
        "        obs1 = torch.tensor([float(a2)], device=device)\n",
        "        obs2 = torch.tensor([float(a1)], device=device)\n",
        "        self.last_actions = (a1, a2)\n",
        "        return (obs1, obs2), r1, r2, done\n",
        "\n",
        "RPS_PAYOFF = [\n",
        "    [[0, 0], [-1, 1], [1, -1]],\n",
        "    [[1, -1], [0, 0], [-1, 1]],\n",
        "    [[-1, 1], [1, -1], [0, 0]]\n",
        "]\n",
        "\n",
        "MP_PAYOFF = [\n",
        "    [[1, -1], [-1, 1]],\n",
        "    [[-1, 1], [1, -1]]\n",
        "]\n",
        "\n",
        "IPD_PAYOFF = [\n",
        "    [[3, 3], [0, 5]],\n",
        "    [[5, 0], [1, 1]]\n",
        "]\n",
        "\n",
        "BOS_PAYOFF = [\n",
        "    [[3, 2], [0, 0]],\n",
        "    [[0, 0], [2, 3]]\n",
        "]\n"
      ],
      "metadata": {
        "id": "8SUmikakhm6o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def solve_minimax(Q_values):\n",
        "    Q_np = Q_values.detach().cpu().numpy()\n",
        "    rows, cols = Q_np.shape\n",
        "    c = np.zeros(rows + 1); c[-1] = -1\n",
        "    A_ub = np.zeros((cols, rows + 1)); A_ub[:, :rows] = -Q_np.T; A_ub[:, -1] = 1\n",
        "    b_ub = np.zeros(cols)\n",
        "    A_eq = np.ones((1, rows + 1)); A_eq[0, -1] = 0; b_eq = np.array([1])\n",
        "    bounds = [(0, 1) for _ in range(rows)] + [(None, None)]\n",
        "    try:\n",
        "        res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n",
        "        if res.success:\n",
        "            probs = res.x[:rows]; probs = np.maximum(probs, 0); probs /= probs.sum()\n",
        "            return torch.tensor(probs, dtype=torch.float32, device=device), res.x[-1]\n",
        "    except: pass\n",
        "    return torch.ones(rows, device=device)/rows, 0.0\n",
        "\n",
        "def solve_ce(Q1, Q2):\n",
        "    Q1_np = Q1.detach().cpu().numpy()\n",
        "    Q2_np = Q2.detach().cpu().numpy()\n",
        "    n_a1, n_a2 = Q1_np.shape\n",
        "\n",
        "    c = -(Q1_np + Q2_np).flatten()\n",
        "\n",
        "    A_ub = []\n",
        "    b_ub = []\n",
        "\n",
        "    for i in range(n_a1):\n",
        "        for k in range(n_a1):\n",
        "            if i == k: continue\n",
        "            row = np.zeros((n_a1, n_a2))\n",
        "            for j in range(n_a2):\n",
        "                row[i, j] = Q1_np[i, j] - Q1_np[k, j]\n",
        "            A_ub.append(-row.flatten())\n",
        "            b_ub.append(0)\n",
        "\n",
        "    for j in range(n_a2):\n",
        "        for k in range(n_a2):\n",
        "            if j == k: continue\n",
        "            row = np.zeros((n_a1, n_a2))\n",
        "            for i in range(n_a1):\n",
        "                row[i, j] = Q2_np[i, j] - Q2_np[i, k]\n",
        "            A_ub.append(-row.flatten())\n",
        "            b_ub.append(0)\n",
        "\n",
        "    A_eq = np.ones((1, n_a1 * n_a2))\n",
        "    b_eq = np.array([1])\n",
        "\n",
        "    bounds = [(0, 1) for _ in range(n_a1 * n_a2)]\n",
        "\n",
        "    try:\n",
        "        res = linprog(c, A_ub=np.array(A_ub), b_ub=np.array(b_ub),\n",
        "                      A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n",
        "        if res.success:\n",
        "            probs = res.x.reshape(n_a1, n_a2)\n",
        "            probs = np.maximum(probs, 0)\n",
        "            probs /= probs.sum()\n",
        "            val1 = np.sum(probs * Q1_np)\n",
        "            val2 = np.sum(probs * Q2_np)\n",
        "            return torch.tensor(probs, dtype=torch.float32, device=device), val1, val2\n",
        "    except: pass\n",
        "\n",
        "    uni = torch.ones((n_a1, n_a2), device=device) / (n_a1 * n_a2)\n",
        "    return uni, 0.0, 0.0\n"
      ],
      "metadata": {
        "id": "IGL4v5JwhopW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MinimaxQAgent:\n",
        "    def __init__(self, n_actions, lr=0.1, gamma=0.99, epsilon=0.1):\n",
        "        self.n_actions = n_actions\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.Q = torch.zeros((n_actions, n_actions), device=device)\n",
        "        self.V = 0.0\n",
        "        self.pi = torch.ones(n_actions, device=device) / n_actions\n",
        "\n",
        "    def get_action(self, obs=None):\n",
        "        if random.random() < self.epsilon:\n",
        "            action = random.randint(0, self.n_actions - 1)\n",
        "        else:\n",
        "            dist = torch.distributions.Categorical(self.pi)\n",
        "            action = dist.sample().item()\n",
        "        return torch.tensor(action, device=device), None, self.pi\n",
        "\n",
        "    def update(self, prev_a, prev_opp_a, reward):\n",
        "        self.pi, self.V = solve_minimax(self.Q)\n",
        "        target = reward + self.gamma * self.V\n",
        "        self.Q[prev_a, prev_opp_a] += self.lr * (target - self.Q[prev_a, prev_opp_a])\n",
        "\n",
        "class CEQAgent:\n",
        "    def __init__(self, n_actions, player_id, lr=0.1, gamma=0.99, epsilon=0.1):\n",
        "        self.n_actions = n_actions\n",
        "        self.player_id = player_id\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.Q_own = torch.zeros((n_actions, n_actions), device=device)\n",
        "        self.Q_opp = torch.zeros((n_actions, n_actions), device=device)\n",
        "        self.joint_pi = torch.ones((n_actions, n_actions), device=device) / (n_actions * n_actions)\n",
        "\n",
        "    def get_action(self, obs=None):\n",
        "        if random.random() < self.epsilon:\n",
        "            action = random.randint(0, self.n_actions - 1)\n",
        "            marginals = torch.ones(self.n_actions, device=device) / self.n_actions\n",
        "        else:\n",
        "            if self.player_id == 0:\n",
        "                marginals = self.joint_pi.sum(dim=1)\n",
        "            else:\n",
        "                marginals = self.joint_pi.sum(dim=0)\n",
        "\n",
        "            if marginals.sum() == 0: marginals = torch.ones(self.n_actions, device=device) / self.n_actions\n",
        "            dist = torch.distributions.Categorical(marginals)\n",
        "            action = dist.sample().item()\n",
        "        return torch.tensor(action, device=device), None, marginals\n",
        "\n",
        "    def update(self, prev_a, prev_opp_a, reward, opp_reward):\n",
        "        self.joint_pi, v_own, v_opp = solve_ce(self.Q_own, self.Q_opp)\n",
        "\n",
        "        target_own = reward + self.gamma * v_own\n",
        "        target_opp = opp_reward + self.gamma * v_opp\n",
        "\n",
        "        self.Q_own[prev_a, prev_opp_a] += self.lr * (target_own - self.Q_own[prev_a, prev_opp_a])\n",
        "        self.Q_opp[prev_a, prev_opp_a] += self.lr * (target_opp - self.Q_opp[prev_a, prev_opp_a])\n",
        "\n",
        "class NashFoFAgent:\n",
        "    def __init__(self, n_actions, algo_type=\"Nash-Q\", lr=0.1, gamma=0.99, epsilon=0.1):\n",
        "        self.n_actions = n_actions\n",
        "        self.algo_type = algo_type\n",
        "        self.lr = lr; self.gamma = gamma; self.epsilon = epsilon\n",
        "        self.Q_own = torch.zeros((n_actions, n_actions), device=device)\n",
        "        self.Q_opp = torch.zeros((n_actions, n_actions), device=device)\n",
        "\n",
        "    def get_action(self, obs=None):\n",
        "        if random.random() < self.epsilon:\n",
        "            action = random.randint(0, self.n_actions - 1)\n",
        "            probs = torch.ones(self.n_actions, device=device) / self.n_actions\n",
        "        else:\n",
        "            if self.algo_type == \"FoF-Q\": probs, _ = solve_minimax(self.Q_own)\n",
        "            else: probs, _ = solve_minimax(self.Q_own)\n",
        "            dist = torch.distributions.Categorical(probs)\n",
        "            action = dist.sample().item()\n",
        "        return torch.tensor(action, device=device), None, probs\n",
        "\n",
        "    def update(self, prev_a, prev_opp_a, reward, opp_reward):\n",
        "        if self.algo_type == \"FoF-Q\": _, next_val_own = solve_minimax(self.Q_own); next_val_opp = 0\n",
        "        else: _, next_val_own = solve_minimax(self.Q_own); _, next_val_opp = solve_minimax(self.Q_opp)\n",
        "\n",
        "        target_own = reward + self.gamma * next_val_own\n",
        "        self.Q_own[prev_a, prev_opp_a] += self.lr * (target_own - self.Q_own[prev_a, prev_opp_a])\n",
        "        if self.algo_type == \"Nash-Q\":\n",
        "            target_opp = opp_reward + self.gamma * next_val_opp\n",
        "            self.Q_opp[prev_a, prev_opp_a] += self.lr * (target_opp - self.Q_opp[prev_a, prev_opp_a])\n",
        "\n",
        "class PPO_ActorCritic(nn.Module):\n",
        "    def __init__(self, actor_obs_dim, critic_obs_dim, act_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.actor = nn.Sequential(nn.Linear(actor_obs_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, act_dim), nn.Softmax(dim=-1))\n",
        "        self.critic = nn.Sequential(nn.Linear(critic_obs_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, 1))\n",
        "    def get_action(self, x):\n",
        "        probs = self.actor(x)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        return action, dist.log_prob(action), probs\n",
        "    def get_value(self, x): return self.critic(x)\n",
        "\n",
        "class PPO_Agent_Wrapper:\n",
        "    def __init__(self, actor_obs_dim, critic_obs_dim, act_dim, lr=0.002, is_mappo=False, pareto_weight=0.0):\n",
        "        self.ac = PPO_ActorCritic(actor_obs_dim, critic_obs_dim, act_dim).to(device)\n",
        "        self.opt = optim.Adam(self.ac.parameters(), lr=lr)\n",
        "        self.is_mappo = is_mappo\n",
        "        self.pareto_weight = pareto_weight\n",
        "\n",
        "    def get_action(self, x):\n",
        "        return self.ac.get_action(x)\n",
        "\n",
        "    def update(self, rollouts):\n",
        "        actor_obs = torch.stack([x[0] for x in rollouts])\n",
        "        critic_obs = torch.stack([x[1] for x in rollouts])\n",
        "        act = torch.tensor([x[2] for x in rollouts], device=device)\n",
        "        ind_r = torch.tensor([x[3] for x in rollouts], device=device)\n",
        "        team_r = torch.tensor([x[4] for x in rollouts], device=device)\n",
        "        old_log_probs = torch.tensor([x[5] for x in rollouts], device=device)\n",
        "\n",
        "        hybrid_reward = (1 - self.pareto_weight) * ind_r + self.pareto_weight * team_r\n",
        "        V = self.ac.get_value(critic_obs).view(-1)\n",
        "        hybrid_reward = hybrid_reward.view(-1)\n",
        "        adv = hybrid_reward - V.detach()\n",
        "\n",
        "        _, _, new_probs = self.ac.get_action(actor_obs)\n",
        "        dist = torch.distributions.Categorical(new_probs)\n",
        "        curr_log_prob = dist.log_prob(act)\n",
        "        ratio = torch.exp(curr_log_prob - old_log_probs)\n",
        "        surr1 = ratio * adv; surr2 = torch.clamp(ratio, 0.8, 1.2) * adv\n",
        "        actor_loss = -torch.min(surr1, surr2).mean()\n",
        "        critic_loss = F.mse_loss(V, hybrid_reward)\n",
        "        self.opt.zero_grad(); (actor_loss + 0.5 * critic_loss).backward(); self.opt.step()\n",
        "\n",
        "class QMixer(nn.Module):\n",
        "    def __init__(self, n_agents, state_dim, embed_dim=32):\n",
        "        super().__init__()\n",
        "        self.n_agents = n_agents; self.embed_dim = embed_dim\n",
        "        self.hyper_w1 = nn.Sequential(nn.Linear(state_dim, embed_dim * n_agents), nn.ReLU())\n",
        "        self.hyper_w2 = nn.Sequential(nn.Linear(state_dim, embed_dim), nn.ReLU())\n",
        "        self.hyper_b1 = nn.Linear(state_dim, embed_dim)\n",
        "        self.hyper_b2 = nn.Sequential(nn.Linear(state_dim, 1), nn.ReLU())\n",
        "    def forward(self, q_values, state):\n",
        "        bs = q_values.size(0)\n",
        "        w1 = torch.abs(self.hyper_w1(state)).view(bs, self.n_agents, self.embed_dim)\n",
        "        b1 = self.hyper_b1(state).view(bs, 1, self.embed_dim)\n",
        "        w2 = torch.abs(self.hyper_w2(state)).view(bs, self.embed_dim, 1)\n",
        "        b2 = self.hyper_b2(state).view(bs, 1, 1)\n",
        "        hidden = F.elu(torch.bmm(q_values.view(bs, 1, self.n_agents), w1) + b1)\n",
        "        q_tot = torch.bmm(hidden, w2) + b2\n",
        "        return q_tot.view(bs, -1)\n",
        "\n",
        "class LolaAgent:\n",
        "    def __init__(self, n_actions, lr=0.1):\n",
        "        self.theta = torch.zeros(n_actions, requires_grad=True, device=device)\n",
        "        self.lr = lr; self.opt = optim.SGD([self.theta], lr=lr)\n",
        "    def get_probs(self): return torch.softmax(self.theta, dim=0)\n",
        "    def step_lola(self, opp_theta, payoff_matrix, opp_lr, is_player_1=True):\n",
        "        p1, p2 = torch.softmax(self.theta, dim=0), torch.softmax(opp_theta, dim=0)\n",
        "        idx_me, idx_opp = (0, 1) if is_player_1 else (1, 0)\n",
        "        V_opp = torch.einsum('i,j,ij->', p1, p2, payoff_matrix[:,:,idx_opp]) if is_player_1 else torch.einsum('i,j,ij->', p2, p1, payoff_matrix[:,:,idx_opp])\n",
        "        grad_opp = torch.autograd.grad(V_opp, opp_theta, create_graph=True)[0]\n",
        "        opp_theta_new = opp_theta + opp_lr * grad_opp\n",
        "        p2_new = torch.softmax(opp_theta_new, dim=0)\n",
        "        if is_player_1: V_lola = torch.einsum('i,j,ij->', p1, p2_new, payoff_matrix[:,:,0])\n",
        "        else: V_lola = torch.einsum('i,j,ij->', p2_new, p1, payoff_matrix[:,:,1])\n",
        "        self.opt.zero_grad(); (-V_lola).backward(); self.opt.step()\n",
        "\n",
        "class BotAgent:\n",
        "    def __init__(self, strategy, action_dim):\n",
        "        self.strategy = strategy; self.action_dim = action_dim\n",
        "    def get_action(self, obs):\n",
        "        opp_last_action = int(obs.item())\n",
        "        if self.strategy == 'Random': act = random.randint(0, self.action_dim - 1)\n",
        "        elif self.strategy == 'Bully': act = 1\n",
        "        elif self.strategy == 'TitForTat': act = opp_last_action\n",
        "        else: act = 0\n",
        "        return torch.tensor(act, device=device), None, None\n",
        "    def update(self, *args): pass\n"
      ],
      "metadata": {
        "id": "9-FB3mT7hp-y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PSRO_Session:\n",
        "    def __init__(self, game_matrix, n_epochs=5, oracle_episodes=50):\n",
        "        self.game_matrix = torch.tensor(game_matrix, dtype=torch.float32, device=device)\n",
        "        self.n_actions = self.game_matrix.shape[0]\n",
        "        self.pop_1 = [torch.ones(self.n_actions, device=device)/self.n_actions]\n",
        "        self.pop_2 = [torch.ones(self.n_actions, device=device)/self.n_actions]\n",
        "        self.n_epochs = n_epochs\n",
        "        self.oracle_episodes = oracle_episodes\n",
        "        self.meta_game_matrix = np.zeros((1, 1, 2))\n",
        "\n",
        "    def evaluate_matchup(self, policy1, policy2):\n",
        "        u1 = torch.einsum('i,j,ij->', policy1, policy2, self.game_matrix[:,:,0])\n",
        "        u2 = torch.einsum('i,j,ij->', policy1, policy2, self.game_matrix[:,:,1])\n",
        "        return u1.item(), u2.item()\n",
        "\n",
        "    def update_meta_game(self):\n",
        "        r1_len = len(self.pop_1)\n",
        "        r2_len = len(self.pop_2)\n",
        "        meta = np.zeros((r1_len, r2_len, 2))\n",
        "        for i in range(r1_len):\n",
        "            for j in range(r2_len):\n",
        "                r1, r2 = self.evaluate_matchup(self.pop_1[i], self.pop_2[j])\n",
        "                meta[i, j, 0] = r1\n",
        "                meta[i, j, 1] = r2\n",
        "        self.meta_game_matrix = meta\n",
        "\n",
        "    def solve_meta_nash(self):\n",
        "        r1, r2 = self.meta_game_matrix[:,:,0], self.meta_game_matrix[:,:,1]\n",
        "\n",
        "        c = np.zeros(len(self.pop_1) + 1); c[-1] = -1\n",
        "        A_ub = np.zeros((len(self.pop_2), len(self.pop_1) + 1))\n",
        "        A_ub[:, :len(self.pop_1)] = -r1.T\n",
        "        A_ub[:, -1] = 1\n",
        "        b_ub = np.zeros(len(self.pop_2))\n",
        "        A_eq = np.ones((1, len(self.pop_1) + 1)); A_eq[0, -1] = 0; b_eq = np.array([1])\n",
        "        bounds = [(0, 1) for _ in range(len(self.pop_1))] + [(None, None)]\n",
        "        try:\n",
        "            res1 = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n",
        "            meta_dist_1 = res1.x[:len(self.pop_1)]\n",
        "        except: meta_dist_1 = np.ones(len(self.pop_1))/len(self.pop_1)\n",
        "\n",
        "        c2 = np.zeros(len(self.pop_2) + 1); c2[-1] = -1\n",
        "        A_ub2 = np.zeros((len(self.pop_1), len(self.pop_2) + 1))\n",
        "        A_ub2[:, :len(self.pop_2)] = -r2\n",
        "        A_ub2[:, -1] = 1\n",
        "        b_ub2 = np.zeros(len(self.pop_1))\n",
        "        A_eq2 = np.ones((1, len(self.pop_2) + 1)); A_eq2[0, -1] = 0; b_eq2 = np.array([1])\n",
        "        bounds2 = [(0, 1) for _ in range(len(self.pop_2))] + [(None, None)]\n",
        "        try:\n",
        "            res2 = linprog(c2, A_ub=A_ub2, b_ub=b_ub2, A_eq=A_eq2, b_eq=b_eq2, bounds=bounds2, method='highs')\n",
        "            meta_dist_2 = res2.x[:len(self.pop_2)]\n",
        "        except: meta_dist_2 = np.ones(len(self.pop_2))/len(self.pop_2)\n",
        "\n",
        "        return torch.tensor(meta_dist_1, dtype=torch.float32, device=device), torch.tensor(meta_dist_2, dtype=torch.float32, device=device)\n",
        "\n",
        "    def train_oracle(self, opp_dist, is_p1):\n",
        "        best_r = -float('inf')\n",
        "        best_pure = 0\n",
        "\n",
        "        if is_p1:\n",
        "            avg_policy_opp = torch.zeros(self.n_actions, device=device)\n",
        "            for idx, prob in enumerate(opp_dist):\n",
        "                avg_policy_opp += prob * self.pop_2[idx]\n",
        "\n",
        "            for a in range(self.n_actions):\n",
        "                pure = torch.zeros(self.n_actions, device=device); pure[a] = 1.0\n",
        "                r, _ = self.evaluate_matchup(pure, avg_policy_opp)\n",
        "                if r > best_r: best_r = r; best_pure = a\n",
        "        else:\n",
        "            avg_policy_opp = torch.zeros(self.n_actions, device=device)\n",
        "            for idx, prob in enumerate(opp_dist):\n",
        "                avg_policy_opp += prob * self.pop_1[idx]\n",
        "\n",
        "            for a in range(self.n_actions):\n",
        "                pure = torch.zeros(self.n_actions, device=device); pure[a] = 1.0\n",
        "                _, r = self.evaluate_matchup(avg_policy_opp, pure)\n",
        "                if r > best_r: best_r = r; best_pure = a\n",
        "\n",
        "        new_pol = torch.zeros(self.n_actions, device=device)\n",
        "        new_pol[best_pure] = 1.0\n",
        "        return new_pol\n",
        "\n",
        "    def run(self):\n",
        "        for epoch in range(self.n_epochs):\n",
        "            self.update_meta_game()\n",
        "            m1, m2 = self.solve_meta_nash()\n",
        "\n",
        "            new_pol_1 = self.train_oracle(m2, is_p1=True)\n",
        "            new_pol_2 = self.train_oracle(m1, is_p1=False)\n",
        "\n",
        "            self.pop_1.append(new_pol_1)\n",
        "            self.pop_2.append(new_pol_2)\n",
        "\n",
        "        return m1, m2\n"
      ],
      "metadata": {
        "id": "FBXF9tnphvyi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(rewards_n):\n",
        "    social_welfare = sum(rewards_n)\n",
        "    diff = abs(rewards_n[0] - rewards_n[1])\n",
        "    total = abs(rewards_n[0]) + abs(rewards_n[1])\n",
        "    fairness_index = 1 - (diff / (total + 1e-8))\n",
        "    return social_welfare, fairness_index\n",
        "\n",
        "def train_matrix_game_generic(algo_type, game_matrix, opponent_type=\"Self\", n_episodes=200, is_one_shot=False, lr=0.01):\n",
        "    steps_per_ep = 1 if is_one_shot else 50\n",
        "    if is_one_shot: n_episodes *= 50\n",
        "\n",
        "    env = MatrixGame(game_matrix, max_steps=steps_per_ep)\n",
        "    n_acts = env.n_actions\n",
        "\n",
        "    if algo_type == \"PSRO\":\n",
        "        psro = PSRO_Session(game_matrix, n_epochs=10)\n",
        "        m1, m2 = psro.run()\n",
        "\n",
        "        p1_atomic = torch.zeros(n_acts, device=device)\n",
        "        for w, pol in zip(m1, psro.pop_1):\n",
        "            p1_atomic += w * pol\n",
        "\n",
        "        p2_atomic = torch.zeros(n_acts, device=device)\n",
        "        for w, pol in zip(m2, psro.pop_2):\n",
        "            p2_atomic += w * pol\n",
        "\n",
        "        return {\n",
        "            'p1_probs': [p1_atomic.cpu().numpy()],\n",
        "            'p2_probs': [p2_atomic.cpu().numpy()],\n",
        "            'social_welfare': [],\n",
        "            'fairness': []\n",
        "        }\n",
        "\n",
        "    history = {'p1_probs': [], 'p2_probs': [], 'social_welfare': [], 'fairness': []}\n",
        "\n",
        "    def create_agent(atype, pid=0):\n",
        "        if atype == \"LOLA\": return LolaAgent(n_acts, lr=lr)\n",
        "        elif atype == \"Minimax-Q\": return MinimaxQAgent(n_acts, lr=lr)\n",
        "        elif atype == \"CE-Q\": return CEQAgent(n_acts, pid, lr=lr)\n",
        "        elif atype in [\"Nash-Q\", \"FoF-Q\"]: return NashFoFAgent(n_acts, algo_type=atype, lr=lr)\n",
        "        else:\n",
        "            is_mappo = (atype == \"MAPPO\")\n",
        "            pareto_w = 0.5 if atype == \"Pareto-AC\" else 0.0\n",
        "            critic_dim = 2 if is_mappo else 1\n",
        "            return PPO_Agent_Wrapper(1, critic_dim, n_acts, lr=lr, is_mappo=is_mappo, pareto_weight=pareto_w)\n",
        "\n",
        "    agent1 = create_agent(algo_type, 0)\n",
        "\n",
        "    if opponent_type == \"Self\":\n",
        "        agent2 = create_agent(algo_type, 1)\n",
        "    else:\n",
        "        agent2 = BotAgent(opponent_type, n_acts)\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        obs1, obs2 = env.reset()\n",
        "\n",
        "        if algo_type == \"LOLA\" and opponent_type == \"Self\":\n",
        "            history['p1_probs'].append(agent1.get_probs().detach().cpu().numpy())\n",
        "            history['p2_probs'].append(agent2.get_probs().detach().cpu().numpy())\n",
        "            agent1.step_lola(agent2.theta, env.payoffs, agent2.lr, True)\n",
        "            agent2.step_lola(agent1.theta, env.payoffs, agent1.lr, False)\n",
        "            continue\n",
        "\n",
        "        ep_r1, ep_r2 = 0, 0\n",
        "        rollout1, rollout2 = [], []\n",
        "\n",
        "        for _ in range(steps_per_ep):\n",
        "            with torch.no_grad():\n",
        "                if algo_type == \"LOLA\": a1 = torch.multinomial(agent1.get_probs(), 1); lp1 = None\n",
        "                else: a1, lp1, _ = agent1.get_action(obs1)\n",
        "\n",
        "                if opponent_type == \"Self\": a2, lp2, _ = agent2.get_action(obs2)\n",
        "                else: a2, _, _ = agent2.get_action(obs2)\n",
        "\n",
        "            (next_o1, next_o2), r1, r2, done = env.step(a1.item(), a2.item())\n",
        "            ep_r1 += r1.item(); ep_r2 += r2.item()\n",
        "\n",
        "            if algo_type == \"Minimax-Q\":\n",
        "                agent1.update(a1.item(), a2.item(), r1)\n",
        "                if opponent_type == \"Self\": agent2.update(a2.item(), a1.item(), r2)\n",
        "\n",
        "            elif algo_type == \"CE-Q\":\n",
        "                agent1.update(a1.item(), a2.item(), r1, r2)\n",
        "                if opponent_type == \"Self\": agent2.update(a2.item(), a1.item(), r2, r1)\n",
        "\n",
        "            elif algo_type in [\"Nash-Q\", \"FoF-Q\"]:\n",
        "                agent1.update(a1.item(), a2.item(), r1, r2)\n",
        "                if opponent_type == \"Self\":\n",
        "                    agent2.update(a2.item(), a1.item(), r2, r1)\n",
        "\n",
        "            elif algo_type != \"LOLA\":\n",
        "                c_obs1 = obs1 if not agent1.is_mappo else torch.cat([obs1, obs2])\n",
        "                rollout1.append((obs1, c_obs1, a1, r1, r1+r2, lp1.detach()))\n",
        "                if opponent_type == \"Self\":\n",
        "                    c_obs2 = obs2 if not agent2.is_mappo else torch.cat([obs2, obs1])\n",
        "                    rollout2.append((obs2, c_obs2, a2, r2, r1+r2, lp2.detach()))\n",
        "\n",
        "            obs1, obs2 = next_o1, next_o2\n",
        "\n",
        "        if algo_type not in [\"LOLA\", \"Nash-Q\", \"FoF-Q\", \"Minimax-Q\", \"CE-Q\"]:\n",
        "            agent1.update(rollout1)\n",
        "            if opponent_type == \"Self\": agent2.update(rollout2)\n",
        "\n",
        "        if algo_type != \"LOLA\":\n",
        "            with torch.no_grad():\n",
        "                _, _, p1 = agent1.get_action(torch.zeros(1, device=device))\n",
        "                history['p1_probs'].append(p1.cpu().numpy())\n",
        "                if opponent_type == \"Self\":\n",
        "                    _, _, p2 = agent2.get_action(torch.zeros(1, device=device))\n",
        "                    history['p2_probs'].append(p2.cpu().numpy())\n",
        "                else:\n",
        "                    history['p2_probs'].append([0.5, 0.5])\n",
        "\n",
        "        w, f = calculate_metrics([ep_r1, ep_r2])\n",
        "        history['social_welfare'].append(w)\n",
        "        history['fairness'].append(f)\n",
        "\n",
        "    return history\n",
        "\n",
        "def train_spread_generic(algo_name, n_episodes=200, noise_level=0.0):\n",
        "    env = simple_spread_v3.parallel_env(N=3, local_ratio=0.5, max_cycles=25, continuous_actions=False)\n",
        "    env.reset(seed=SEED)\n",
        "    n_agents = 3\n",
        "    obs_dim = 18; act_dim = 5\n",
        "    history = {'rewards': [], 'social_welfare': [], 'fairness': []}\n",
        "\n",
        "    if algo_name == \"QMIX\":\n",
        "        q_nets = [nn.Sequential(nn.Linear(obs_dim, 64), nn.ReLU(), nn.Linear(64, act_dim)).to(device) for _ in range(n_agents)]\n",
        "        mixer = QMixer(n_agents, state_dim=obs_dim*n_agents).to(device)\n",
        "        optimizer = optim.Adam(list(mixer.parameters()) + [p for net in q_nets for p in net.parameters()], lr=0.001)\n",
        "    else:\n",
        "        is_mappo = (algo_name == \"MAPPO\")\n",
        "        pareto_w = 0.5 if algo_name == \"Pareto-AC\" else 0.0\n",
        "        critic_dim = obs_dim * n_agents if is_mappo else obs_dim\n",
        "        agents = [PPO_Agent_Wrapper(obs_dim, critic_dim, act_dim, lr=0.001, is_mappo=is_mappo, pareto_weight=pareto_w) for _ in range(n_agents)]\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        obs_dict, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        agent_rewards = {a: 0 for a in env.agents}\n",
        "        episode_data_qmix = []\n",
        "        rollouts_ppo = [[] for _ in range(n_agents)]\n",
        "\n",
        "        while env.agents:\n",
        "            agent_ids = env.agents\n",
        "            obs_tensors = []\n",
        "            for a in agent_ids:\n",
        "                raw = torch.tensor(obs_dict[a], dtype=torch.float32, device=device)\n",
        "                if noise_level > 0: raw += torch.randn_like(raw) * noise_level\n",
        "                obs_tensors.append(raw)\n",
        "            global_state = torch.cat(obs_tensors) if obs_tensors else torch.zeros(1, device=device)\n",
        "\n",
        "            actions = {}; q_vals = []\n",
        "\n",
        "            if algo_name == \"QMIX\":\n",
        "                for i, agent in enumerate(agent_ids):\n",
        "                    q = q_nets[i](obs_tensors[i])\n",
        "                    q_vals.append(q)\n",
        "                    if random.random() < max(0.05, 1 - ep/150): act = env.action_space(agent).sample()\n",
        "                    else: act = torch.argmax(q).item()\n",
        "                    actions[agent] = act\n",
        "                episode_data_qmix.append((global_state, torch.stack(q_vals), actions, list(agent_ids)))\n",
        "            else:\n",
        "                for i, agent in enumerate(agent_ids):\n",
        "                    a, lp, _ = agents[i].ac.get_action(obs_tensors[i])\n",
        "                    actions[agent] = a.item()\n",
        "\n",
        "            next_obs, rewards, terminations, truncations, _ = env.step(actions)\n",
        "            team_r = sum(rewards.values())\n",
        "            total_reward += team_r\n",
        "            for a in agent_ids: agent_rewards[a] += rewards[a]\n",
        "\n",
        "            if algo_name != \"QMIX\":\n",
        "                for i, agent in enumerate(agent_ids):\n",
        "                    if agent in next_obs:\n",
        "                        old_lp = agents[i].ac.get_action(obs_tensors[i])[1].detach()\n",
        "                        c_obs = global_state if agents[i].is_mappo else obs_tensors[i]\n",
        "                        rollouts_ppo[i].append((obs_tensors[i], c_obs, torch.tensor(actions[agent], device=device),\n",
        "                                                torch.tensor(rewards[agent], device=device), torch.tensor(team_r, device=device), old_lp))\n",
        "            obs_dict = next_obs\n",
        "\n",
        "        if algo_name == \"QMIX\":\n",
        "            R = torch.tensor(total_reward, device=device)\n",
        "            loss = 0\n",
        "            for (state, qs, acts, step_agents) in episode_data_qmix:\n",
        "                chosen_qs = [qs[i][acts[agent]] for i, agent in enumerate(step_agents)]\n",
        "                if chosen_qs:\n",
        "                    q_tot = mixer(torch.stack(chosen_qs).unsqueeze(0), state.unsqueeze(0))\n",
        "                    loss += (q_tot - R)**2\n",
        "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        else:\n",
        "            for i in range(n_agents):\n",
        "                if rollouts_ppo[i]: agents[i].update(rollouts_ppo[i])\n",
        "\n",
        "        history['rewards'].append(total_reward)\n",
        "        w, f = calculate_metrics(list(agent_rewards.values()))\n",
        "        history['social_welfare'].append(w)\n",
        "        history['fairness'].append(f)\n",
        "\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "Lhoaj_bEhzud"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_store = {}\n",
        "\n",
        "print(\"--- EXP A: Zero-Sum Games ---\")\n",
        "\n",
        "print(\"Running MP: Minimax-Q, CE-Q, PSRO, Nash-Q, IPPO, FoF-Q, LOLA...\")\n",
        "results_store['mp_minimax'] = train_matrix_game_generic('Minimax-Q', MP_PAYOFF, n_episodes=150)\n",
        "results_store['mp_ceq'] = train_matrix_game_generic('CE-Q', MP_PAYOFF, n_episodes=150)\n",
        "results_store['mp_psro'] = train_matrix_game_generic('PSRO', MP_PAYOFF)\n",
        "results_store['mp_nash'] = train_matrix_game_generic('Nash-Q', MP_PAYOFF, n_episodes=150)\n",
        "results_store['mp_ippo'] = train_matrix_game_generic('IPPO', MP_PAYOFF, n_episodes=150)\n",
        "results_store['mp_fof'] = train_matrix_game_generic('FoF-Q', MP_PAYOFF, n_episodes=150)\n",
        "results_store['mp_lola'] = train_matrix_game_generic('LOLA', MP_PAYOFF, n_episodes=150, lr=0.1)\n",
        "results_store['mp_mappo'] = train_matrix_game_generic('MAPPO', MP_PAYOFF, n_episodes=150)\n",
        "\n",
        "print(\"Running RPS: Minimax-Q, CE-Q, PSRO, Nash-Q, IPPO, MAPPO, FoF-Q, LOLA...\")\n",
        "results_store['rps_minimax'] = train_matrix_game_generic('Minimax-Q', RPS_PAYOFF, n_episodes=200)\n",
        "results_store['rps_ceq'] = train_matrix_game_generic('CE-Q', RPS_PAYOFF, n_episodes=200)\n",
        "results_store['rps_psro'] = train_matrix_game_generic('PSRO', RPS_PAYOFF)\n",
        "results_store['rps_nash'] = train_matrix_game_generic('Nash-Q', RPS_PAYOFF, n_episodes=200)\n",
        "results_store['rps_ippo'] = train_matrix_game_generic('IPPO', RPS_PAYOFF, n_episodes=200)\n",
        "results_store['rps_mappo'] = train_matrix_game_generic('MAPPO', RPS_PAYOFF, n_episodes=200)\n",
        "results_store['rps_fof'] = train_matrix_game_generic('FoF-Q', RPS_PAYOFF, n_episodes=200)\n",
        "results_store['rps_lola'] = train_matrix_game_generic('LOLA', RPS_PAYOFF, n_episodes=200, lr=0.1)\n",
        "\n",
        "\n",
        "print(\"\\n--- EXP B: Mixed-Motive Games ---\")\n",
        "\n",
        "print(\"Running IPD: CE-Q, Minimax-Q, PSRO, Nash-Q, FoF-Q, IPPO, MAPPO, LOLA, Pareto-AC...\")\n",
        "results_store['ipd_ceq'] = train_matrix_game_generic('CE-Q', IPD_PAYOFF, n_episodes=200)\n",
        "results_store['ipd_minimax'] = train_matrix_game_generic('Minimax-Q', IPD_PAYOFF, n_episodes=200)\n",
        "results_store['ipd_psro'] = train_matrix_game_generic('PSRO', IPD_PAYOFF)\n",
        "results_store['ipd_nash'] = train_matrix_game_generic('Nash-Q', IPD_PAYOFF, n_episodes=200)\n",
        "results_store['ipd_fof'] = train_matrix_game_generic('FoF-Q', IPD_PAYOFF, n_episodes=200)\n",
        "results_store['ipd_ippo'] = train_matrix_game_generic('IPPO', IPD_PAYOFF, n_episodes=200)\n",
        "results_store['ipd_mappo'] = train_matrix_game_generic('MAPPO', IPD_PAYOFF, n_episodes=200)\n",
        "results_store['ipd_lola'] = train_matrix_game_generic('LOLA', IPD_PAYOFF, n_episodes=200, lr=0.1)\n",
        "results_store['ipd_pac'] = train_matrix_game_generic('Pareto-AC', IPD_PAYOFF, n_episodes=200)\n",
        "\n",
        "bot_list = ['TitForTat', 'Random', 'Bully']\n",
        "for bot in bot_list:\n",
        "    print(f\"Running IPD vs {bot}...\")\n",
        "    results_store[f'ipd_vs_{bot}_ippo'] = train_matrix_game_generic('IPPO', IPD_PAYOFF, opponent_type=bot, n_episodes=150)\n",
        "    results_store[f'ipd_vs_{bot}_nash'] = train_matrix_game_generic('Nash-Q', IPD_PAYOFF, opponent_type=bot, n_episodes=150)\n",
        "    results_store[f'ipd_vs_{bot}_minimax'] = train_matrix_game_generic('Minimax-Q', IPD_PAYOFF, opponent_type=bot, n_episodes=150)\n",
        "    results_store[f'ipd_vs_{bot}_ceq'] = train_matrix_game_generic('CE-Q', IPD_PAYOFF, opponent_type=bot, n_episodes=150)\n",
        "    results_store[f'ipd_vs_{bot}_fof'] = train_matrix_game_generic('FoF-Q', IPD_PAYOFF, opponent_type=bot, n_episodes=150)\n",
        "\n",
        "print(\"Running BoS: CE-Q, Minimax-Q, PSRO, Nash-Q, IPPO, FoF-Q...\")\n",
        "results_store['bos_ceq'] = train_matrix_game_generic('CE-Q', BOS_PAYOFF, n_episodes=200)\n",
        "results_store['bos_minimax'] = train_matrix_game_generic('Minimax-Q', BOS_PAYOFF, n_episodes=200)\n",
        "results_store['bos_psro'] = train_matrix_game_generic('PSRO', BOS_PAYOFF)\n",
        "results_store['bos_nash'] = train_matrix_game_generic('Nash-Q', BOS_PAYOFF, n_episodes=200)\n",
        "results_store['bos_ippo'] = train_matrix_game_generic('IPPO', BOS_PAYOFF, n_episodes=200)\n",
        "results_store['bos_fof'] = train_matrix_game_generic('FoF-Q', BOS_PAYOFF, n_episodes=200)\n",
        "\n",
        "print(\"Running IPD (One-Shot): IPPO, Nash-Q, Minimax-Q, CE-Q, FoF-Q, LOLA...\")\n",
        "\n",
        "results_store['pd_oneshot_ippo'] = train_matrix_game_generic('IPPO', IPD_PAYOFF, n_episodes=200, is_one_shot=True)\n",
        "results_store['pd_oneshot_nash'] = train_matrix_game_generic('Nash-Q', IPD_PAYOFF, n_episodes=200, is_one_shot=True)\n",
        "results_store['pd_oneshot_minimax'] = train_matrix_game_generic('Minimax-Q', IPD_PAYOFF, n_episodes=200, is_one_shot=True)\n",
        "results_store['pd_oneshot_ceq'] = train_matrix_game_generic('CE-Q', IPD_PAYOFF, n_episodes=200, is_one_shot=True)\n",
        "results_store['pd_oneshot_fof'] = train_matrix_game_generic('FoF-Q', IPD_PAYOFF, n_episodes=200, is_one_shot=True)\n",
        "results_store['pd_oneshot_lola'] = train_matrix_game_generic('LOLA', IPD_PAYOFF, n_episodes=200, is_one_shot=True, lr=0.1)\n",
        "\n",
        "\n",
        "print(\"\\n--- EXP C: Simple Spread (Cooperative) ---\")\n",
        "\n",
        "print(\"Running Spread: IPPO, MAPPO, QMIX, Pareto-AC...\")\n",
        "results_store['spread_ippo'] = train_spread_generic('IPPO', n_episodes=150)\n",
        "results_store['spread_mappo'] = train_spread_generic('MAPPO', n_episodes=150)\n",
        "results_store['spread_qmix'] = train_spread_generic('QMIX', n_episodes=150)\n",
        "results_store['spread_pac'] = train_spread_generic('Pareto-AC', n_episodes=150)\n",
        "\n",
        "\n",
        "print(\"Running Spread (Noisy - Robustness): MAPPO, QMIX...\")\n",
        "results_store['spread_ippo_noisy'] = train_spread_generic('IPPO', n_episodes=150, noise_level=0.5)\n",
        "results_store['spread_mappo_noisy'] = train_spread_generic('MAPPO', n_episodes=150, noise_level=0.5)\n",
        "results_store['spread_qmix_noisy'] = train_spread_generic('QMIX', n_episodes=150, noise_level=0.5)\n",
        "results_store['spread_pac_noisy'] = train_spread_generic('Pareto-AC', n_episodes=150, noise_level=0.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwMPVC4Qh6Gm",
        "outputId": "4fb15d88-d4e6-421f-c76c-854462702b51"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- EXP A: Zero-Sum Games ---\n",
            "Running MP: Minimax-Q, CE-Q, PSRO, Nash-Q, IPPO, FoF-Q, LOLA...\n",
            "Running RPS: Minimax-Q, CE-Q, PSRO, Nash-Q, IPPO, MAPPO, FoF-Q, LOLA...\n",
            "\n",
            "--- EXP B: Mixed-Motive Games ---\n",
            "Running IPD: CE-Q, Minimax-Q, PSRO, Nash-Q, FoF-Q, IPPO, MAPPO, LOLA, Pareto-AC...\n",
            "Running IPD vs TitForTat...\n",
            "Running IPD vs Random...\n",
            "Running IPD vs Bully...\n",
            "Running BoS: CE-Q, Minimax-Q, PSRO, Nash-Q, IPPO, FoF-Q...\n",
            "Running IPD (One-Shot): IPPO, Nash-Q, Minimax-Q, CE-Q, FoF-Q, LOLA...\n",
            "\n",
            "--- EXP C: Simple Spread (Cooperative) ---\n",
            "Running Spread: IPPO, MAPPO, QMIX, Pareto-AC...\n",
            "Running Spread (Noisy - Robustness): MAPPO, QMIX...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "save_path = '/content/drive/My Drive/MARL_Final_Project_Full_Revised'\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "np.savez(f'{save_path}/complete_results.npz', **results_store)\n",
        "print(f\"Results saved to {save_path}/complete_results.npz\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TBYIddpiLpH",
        "outputId": "e24f3e5e-265b-4ca2-8023-9b9fd140abf8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Results saved to /content/drive/My Drive/MARL_Final_Project_Full_Revised/complete_results.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "load_path = '/content/drive/My Drive/MARL_Final_Project_Full_Revised/complete_results.npz'\n",
        "save_path = '/content/drive/My Drive/MARL_Final_Project_Full_Revised/MARL_Metrics_Analysis.xlsx'\n",
        "\n",
        "print(f\"Loading results from: {load_path}\")\n",
        "try:\n",
        "    data = np.load(load_path, allow_pickle=True)\n",
        "    results_store = {key: data[key].item() for key in data}\n",
        "    print(\"Data successfully loaded and converted to dictionary format.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: File not found! Please run the training code first.\")\n",
        "    results_store = {}\n",
        "\n",
        "def calculate_metrics(results, window=20):\n",
        "    metrics_list = []\n",
        "\n",
        "    game_groups = {\n",
        "        'Matching Pennies': ['mp_minimax', 'mp_ceq', 'mp_psro', 'mp_nash', 'mp_ippo', 'mp_fof', 'mp_lola', 'mp_mappo'],\n",
        "        'RPS': ['rps_minimax', 'rps_ceq', 'rps_psro', 'rps_nash', 'rps_ippo', 'rps_mappo', 'rps_fof', 'rps_lola'],\n",
        "        'Iterated PD': ['ipd_ceq', 'ipd_minimax', 'ipd_psro', 'ipd_nash', 'ipd_fof', 'ipd_ippo', 'ipd_mappo', 'ipd_lola', 'ipd_pac'],\n",
        "        'IPD vs Bots': [k for k in results.keys() if 'ipd_vs_' in k],\n",
        "        'BoS': ['bos_ceq', 'bos_minimax', 'bos_psro', 'bos_nash', 'bos_ippo', 'bos_fof'],\n",
        "        'Spread (Coop)': ['spread_ippo', 'spread_mappo', 'spread_qmix', 'spread_pac',\n",
        "                          'spread_ippo_noisy', 'spread_mappo_noisy', 'spread_qmix_noisy', 'spread_pac_noisy'],\n",
        "        'One-Shot PD': ['pd_oneshot_ippo', 'pd_oneshot_nash', 'pd_oneshot_minimax', 'pd_oneshot_ceq', 'pd_oneshot_fof', 'pd_oneshot_lola']\n",
        "    }\n",
        "\n",
        "    final_dfs = {}\n",
        "\n",
        "    for group_name, keys in game_groups.items():\n",
        "        rows = []\n",
        "        for key in keys:\n",
        "            if key not in results:\n",
        "                continue\n",
        "\n",
        "            item = results[key]\n",
        "            algo_name = key.split('_')[-1].upper()\n",
        "            if 'noisy' in key: algo_name += \" (Noisy)\"\n",
        "            if 'vs' in key: algo_name = key\n",
        "\n",
        "            p1_probs = np.array(item.get('p1_probs', []))\n",
        "\n",
        "            welfare = np.array(item.get('social_welfare', []))\n",
        "            fairness = np.array(item.get('fairness', []))\n",
        "            rewards = np.array(item.get('rewards', []))\n",
        "\n",
        "            stats = {'Algorithm': algo_name}\n",
        "\n",
        "            if len(p1_probs) > 0:\n",
        "                if len(p1_probs) == 1:\n",
        "                    stability = 0.0\n",
        "                    final_prob_dist = p1_probs[0]\n",
        "                else:\n",
        "                    last_chunk = p1_probs[-window:]\n",
        "                    stability = np.mean(np.std(last_chunk, axis=0))\n",
        "                    final_prob_dist = np.mean(last_chunk, axis=0)\n",
        "\n",
        "                stats['Convergence Score (Lower is Better)'] = round(stability, 5)\n",
        "                stats['Dominant Action Prob'] = round(np.max(final_prob_dist), 4)\n",
        "\n",
        "            elif len(rewards) > 0:\n",
        "                last_rewards = rewards[-window:]\n",
        "                stats['Convergence Score (Lower is Better)'] = round(np.std(last_rewards), 4)\n",
        "                stats['Dominant Action Prob'] = \"N/A\"\n",
        "\n",
        "            if len(welfare) > 0:\n",
        "                stats['Avg Final Welfare'] = round(np.mean(welfare[-window:]), 4)\n",
        "            elif len(rewards) > 0:\n",
        "                stats['Avg Final Welfare'] = round(np.mean(rewards[-window:]), 4)\n",
        "            else:\n",
        "                stats['Avg Final Welfare'] = 0.0\n",
        "\n",
        "            if len(fairness) > 0:\n",
        "                stats['Avg Final Fairness'] = round(np.mean(fairness[-window:]), 4)\n",
        "            else:\n",
        "                stats['Avg Final Fairness'] = \"N/A\"\n",
        "\n",
        "            rows.append(stats)\n",
        "\n",
        "        if rows:\n",
        "            final_dfs[group_name] = pd.DataFrame(rows)\n",
        "\n",
        "    return final_dfs\n",
        "\n",
        "print(\"Calculating analysis...\")\n",
        "if results_store:\n",
        "    dfs = calculate_metrics(results_store)\n",
        "\n",
        "    with pd.ExcelWriter(save_path, engine='openpyxl') as writer:\n",
        "        for sheet_name, df in dfs.items():\n",
        "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "            worksheet = writer.sheets[sheet_name]\n",
        "            for column in df:\n",
        "                column_width = max(df[column].astype(str).map(len).max(), len(column))\n",
        "                col_idx = df.columns.get_loc(column)\n",
        "                worksheet.column_dimensions[chr(65 + col_idx)].width = column_width + 2\n",
        "\n",
        "    print(f\"\\nSUCCESS: All analyses saved to Excel file:\\n{save_path}\")\n",
        "    print(\"\\nExcel File Content (Sheet Names):\")\n",
        "    for sheet in dfs.keys():\n",
        "        print(f\"- {sheet}\")\n",
        "else:\n",
        "    print(\"No data to process.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5k0mcJ2uIW8",
        "outputId": "ffbd770a-c655-4b84-a6f2-8ba187831399"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading results from: /content/drive/My Drive/MARL_Final_Project_Full_Revised/complete_results.npz\n",
            "Data successfully loaded and converted to dictionary format.\n",
            "Calculating analysis...\n",
            "\n",
            "SUCCESS: All analyses saved to Excel file:\n",
            "/content/drive/My Drive/MARL_Final_Project_Full_Revised/MARL_Metrics_Analysis.xlsx\n",
            "\n",
            "Excel File Content (Sheet Names):\n",
            "- Matching Pennies\n",
            "- RPS\n",
            "- Iterated PD\n",
            "- IPD vs Bots\n",
            "- BoS\n",
            "- Spread (Coop)\n",
            "- One-Shot PD\n"
          ]
        }
      ]
    }
  ]
}